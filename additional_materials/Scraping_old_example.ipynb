{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Basic Example\n",
    "## 1. Searching HTML with `BeautifulSoup`\n",
    "\n",
    "BeautifulSoup is a Python library for parsing HTML documents. We can use it to extract data from HTML we fetch from the web but first we'll try it out with a simple HTML example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <h1> BeautifulSoup </h1>\n",
    "    <p> BeautifulSoup is a Python library for parsing HTML documents. We can use it to extract data from HTML we fetch\n",
    "        from the web.\n",
    "        Here, we're just using it to parse some simple sample HTML. </p>\n",
    "\n",
    "    <h2 class=\"important\"> Searching the tree </h2>\n",
    "    <p id=\"searching_description\" style=\"color: red\"> BeautifulSoup allows us to search the HTML tree in lots of different ways: by tag, by\n",
    "        id, by CSS class, and so on. </p>\n",
    "    <ol>\n",
    "        <li> By tag: we could search for every li </li>\n",
    "        <li> By id: we could search for the p tag with id=\"searching_description\" </li>\n",
    "        <li class=\"important\"> By class: we could search for every tag with a given class </li>\n",
    "    </ol>\n",
    "\n",
    "<body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At present the `sample_html` is just a string. In order to search through it, we'll create a Soup object with beautifulSoup - a searchable object representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(sample_html, 'html.parser') # Parsing the HTML using BeautifulSoup and the built-in HTML parser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now search the `soup` object to extract useful information. Let's try searching by tag, id, and class:\n",
    "\n",
    "**First, by tag:** let's find the text of every list element (`<li>`) on the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " By tag: we could search for every li \n",
      " By id: we could search for the p tag with id=\"searching_description\" \n",
      " By class: we could search for every tag with a given class \n"
     ]
    }
   ],
   "source": [
    "list_elements = soup.find_all('li') # Finding all the list elements in the HTML\n",
    "for element in list_elements:\n",
    "    print(element.text) # Printing the text of each list element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, by id** let's find what the paragraph (`<p>`) with the `id` `searching_description` says and let's also take a look at its style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  BeautifulSoup allows us to search the HTML tree in lots of different ways: by tag, by\n",
      "        id, by CSS class, and so on. \n",
      "style: color: red\n"
     ]
    }
   ],
   "source": [
    "# instead of using find_all, we can use find to find the first element that matches the search criteria\n",
    "description_element = soup.find('p', {'id': 'searching_description'}) # Finding the paragraph with id=\"searching_description\"\n",
    "description_element\n",
    "\n",
    "print('text:', description_element.text) # Printing the text of the paragraph with id=\"searching_description\"\n",
    "print('style:', description_element['style']) # Printing the value of the style attribute of the paragraph with id=\"searching_description\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, by class:** let's find all elements with the `class` `important`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 elements with class='important'\n",
      "<h2 class=\"important\"> Searching the tree </h2>\n",
      "<li class=\"important\"> By class: we could search for every tag with a given class </li>\n"
     ]
    }
   ],
   "source": [
    "important_elements = soup.find_all(class_='important') # Finding all the elements with class=\"important\"\n",
    "print(f\"There are {len(important_elements)} elements with class='important'\")\n",
    "for element in important_elements:\n",
    "    print(element) # Printing each element with class=\"important\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting HTML from the web with `Requests`\n",
    "\n",
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <div style=\"flex: 0 2 auto;\">\n",
    "        <img src=\"https://raw.githubusercontent.com/FM-ds/ScrapingWorkshop/main/notebook_images/sample_html_safari.png\" width=\"400px\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1 1 auto; margin-top: 10px; margin-right: 150px\">\n",
    "        <p>We just extracted information from HTML which was defined locally in a string <code>sample_html</code>. Usually, we care about HTML found on the internet. As a simple example, the page defined in <code>sample_html</code> is available at <a href=\"http://www.fmcevoy.io/ScrapingWorkshop/sample_html\">http://www.fmcevoy.io/ScrapingWorkshop/sample_html</a></p>\n",
    "        <p>To download HTML (and any other resources) from the internet, we can use the <code>requests</code> module.</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<body>\\n    <h1> BeautifulSoup </h1>\\n    <p> BeautifulSoup is a Python library for parsing HTML documents. We can use it to extract data from HTML we fetch\\n        from the web.\\n        Here, we\\'re just using it to parse some simple sample HTML. </p>\\n\\n    <h2 class=\"important\"> Searching the tree </h2>\\n    <p id=\"searching_description\" style=\"color: red\"> BeautifulSoup allows us to search the HTML tree in lots of different ways: by tag, by\\n        id, by CSS class, and so on. </p>\\n    <ol>\\n        <li> By tag: we could search for every li </li>\\n        <li> By id: we could search for the p tag with id=\"searching_description\" </li>\\n        <li class=\"important\"> By class: we could search for every tag with a given class </li>\\n    </ol>\\n\\n<body>\\n</html>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req = requests.get('http://www.fmcevoy.io/ScrapingWorkshop/sample_html') # Making a request for the sample HTML hosted on the web\n",
    "req.text # looking at the text of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then make a traversable `soup` object using the response and search as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li> By tag: we could search for every li </li>,\n",
       " <li> By id: we could search for the p tag with id=\"searching_description\" </li>,\n",
       " <li class=\"important\"> By class: we could search for every tag with a given class </li>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(req.text, 'html.parser') # Instead of using the sample HTML, we're using the HTML from the web that we fetched in the previous step\n",
    "soup.find_all('li') # Finding all the list elements in the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Useful Example: Scraping Prices from M&S\n",
    "\n",
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <div style=\"flex: 0 1.75 auto; \">\n",
    "        <img src=\"https://raw.githubusercontent.com/FM-ds/ScrapingWorkshop/main/notebook_images/ms_plants.png\" width=\"500px\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1 1 auto; margin-top: 10px; margin-right: 150px\">\n",
    "        Scraping the sample html is a basic introduction. Let's move to collecting some real-world data.\n",
    "\n",
    "Marks and Spencers is a high-end British supermarket. Let's see if we can collect prices from one of their pages. Josh and I scrape around 100K prices every day to investigate inflation but there are lots of other uses for this data as well:\n",
    "\n",
    "- **Price Comparisons**: You could make a supermarket vs supermarket price comparison tool\n",
    "- **Market Research**: Maybe you want to investigate pricing of your competitors\n",
    "- **Procurement**: Waiting for sales to emerge\n",
    "\n",
    "Today, we'll try scraping prices from just one page, Marks and Spencers' plants: https://www.marksandspencer.com/l/flowers-and-plants/plants?sort=best_seller+desc \n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Structure with Inspect Element\n",
    "\n",
    "Before we write any code, we should use inspect element to see how prices, pictures and more are defined in the HTML.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/FM-ds/ScrapingWorkshop/main/notebook_images/ms_inspect_element.png\" width=\"800vw\">\n",
    "\n",
    "There are lots of elements and intimidating details but here we can see:\n",
    "\n",
    "- Product Details are contained in `<div>` elements with a `data-tagg` attribute that starts with `product-card-`\n",
    "- Product Titles are found in `<h2>` elements within the `product-card-` divs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Extracting Product Info\n",
    "\n",
    "Let's put together these two facts to find all the product titles. \n",
    "First, we'll download the HTML from the M&S site and then we'll search for all the product cards to find their titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.marksandspencer.com/l/flowers-and-plants/plants?sort=best_seller+desc\"\n",
    "req = requests.get(url) # Making a request for the M&S plants page\n",
    "soup = BeautifulSoup(req.text, 'html.parser') # Parsing the HTML using BeautifulSoup and the built-in HTML parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Titles\n",
    "\n",
    "To find all the product titles, first we'll build a list of all divs that contain product details by searching for all elements with the data-tagg attribute that contain \"product-card\".\n",
    "\n",
    "For each of these product divs, we'll then search for the title (a `h2` element) and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oriental Lily\n",
      "Spring Flowering Basket\n",
      "Rose Trough\n",
      "Yellow Miniature Phalaenopsis Orchid in Ceramic Pot\n",
      "White Miniature Phalaenopsis Orchid in Ceramic Pot\n"
     ]
    }
   ],
   "source": [
    "# find all divs with data-tagg attribute that contain \"product-card\" and potentially other values\n",
    "product_divs = soup.find_all('div', attrs={'data-tagg': lambda x: x and \"product-card\" in x}) # Using a lambda function to find all divs with data-tagg attribute that contain \"product-card\" and potentially other values\n",
    "for product_div in product_divs[:5]: # Looping through the first 5 product divs\n",
    "    name = product_div.find('h2').text # Finding the h2 tag within the div and getting its text\n",
    "    print(name) # Printing the name of the product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Prices\n",
    "\n",
    "Returning to inspect element, we can see that prices are defined in `spans` within the product divs. There are lots of spans, so the best way is to search through all the spans and just keep the one that starts with a `£` sign.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/FM-ds/ScrapingWorkshop/main/notebook_images/ms_inspect_price.png\" width=\"800vw\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oriental Lily: £25\n",
      "Spring Flowering Basket: £30\n",
      "Rose Trough: £25\n",
      "Yellow Miniature Phalaenopsis Orchid in Ceramic Pot: £20\n",
      "White Miniature Phalaenopsis Orchid in Ceramic Pot: £20\n"
     ]
    }
   ],
   "source": [
    "# find all divs with data-tagg attribute that contain \"product-card\" and potentially other values\n",
    "product_divs = soup.find_all('div', attrs={'data-tagg': lambda x: x and \"product-card\" in x}) # Using a lambda function to find all divs with data-tagg attribute that contain \"product-card\" and potentially other values\n",
    "for product_div in product_divs[:5]: # Looping through the first 5 product divs\n",
    "    name = product_div.find('h2').text # Finding the h2 tag within the div and getting its text\n",
    "\n",
    "    # Finding the price of the product\n",
    "    ## First find all spans in the product div\n",
    "    spans = product_div.find_all('span')\n",
    "    ## We want to find only the span with text that starts with \"£\"\n",
    "    ## To to this we'll filter the list of spans for only those that start with \"£\" and take the first one\n",
    "    for span in spans:\n",
    "        if span.text.startswith(\"£\"):\n",
    "            price = span.text\n",
    "            break # Once we've found the price, we can stop looking\n",
    "\n",
    "    print(f\"{name}: {price}\") # Printing the name and price of the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Reviews \n",
    "\n",
    "Finally, we'll find the average review and review counts. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/FM-ds/ScrapingWorkshop/main/notebook_images/ms_inspect_rating.png\" width=\"800vw\">\n",
    "\n",
    "Going back to inspect element, we can see that reviews and review counts are contained as spans in a `<button>` element:\n",
    "- We can find the rating (4.6) by looking for spans with text that are numeric\n",
    "- We can find the review count by looking for spans that have text-decoration=\"underline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oriental Lily: £25\n",
      "    Rating: 4.6 (1759 reviews)\n",
      "Spring Flowering Basket: £30\n",
      "    Rating: 4.4 (968 reviews)\n",
      "Rose Trough: £25\n",
      "    Rating: 4.5 (1020 reviews)\n",
      "Yellow Miniature Phalaenopsis Orchid in Ceramic Pot: £20\n",
      "    Rating: 4.6 (573 reviews)\n",
      "White Miniature Phalaenopsis Orchid in Ceramic Pot: £20\n",
      "    Rating: 4.7 (477 reviews)\n"
     ]
    }
   ],
   "source": [
    "product_divs = soup.find_all('div', attrs={'data-tagg': lambda x: x and \"product-card\" in x}) # Using a lambda function to find all divs with data-tagg attribute that contain \"product-card\" and potentially other values\n",
    "for product_div in product_divs[:5]: # Looping through the first 5 product divs\n",
    "    name = product_div.find('h2').text # Finding the h2 tag within the div and getting its text\n",
    "\n",
    "    # Finding the price of the product\n",
    "    ## First find all spans in the product div\n",
    "    spans = product_div.find_all('span')\n",
    "    ## We want to find only the span with text that starts with \"£\"\n",
    "    ## To to this we'll filter the list of spans for only those that start with \"£\" and take the first one\n",
    "    for span in spans:\n",
    "        if span.text.startswith(\"£\"):\n",
    "            price = span.text\n",
    "            break # Once we've found the price, we can stop looking\n",
    "\n",
    "    # Finding Ratings\n",
    "    # Next, we'll find the first button element and loop over the spans to find \n",
    "    rating_btn = product_div.find('button')\n",
    "    if rating_btn:\n",
    "        spans = rating_btn.find_all('span')\n",
    "        rating = spans[0].text # The first span contains the rating\n",
    "        rating_count = spans[-1].text # The last span contains the rating count\n",
    "\n",
    "    else:\n",
    "        rating = None\n",
    "        rating_count = None\n",
    "\n",
    "    print(f\"{name}: {price}\") # Printing the name and price of the product\n",
    "    print(f\"    Rating: {rating} ({rating_count})\") # Printing the rating and rating count of the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
